---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Scraping Web (Raspado web)

La definición del scraping web que se toma en este documento proviene de [@Mitchell2015] que expresa:

> El Web Scraping es la **recolección automática** de *información* de los *sitios web*. (obviamente no a través de un humano usando un navegador web).

Un tema dentro del scraping web son las denominadas *APIs* (Application Programming Interface), *estas son entradas a las páginas web diseñadas por los administradores de la página web*, **por lo mismo no siempre contienen toda la información que se desea**. Aunque las API no son tan *ubicuas* como deberían, puede encontrar API para muchos tipos de información. Interesado en la música? Hay algunas API diferentes que pueden darle canciones, artistas, álbumes e incluso información sobre estilos musicales y artistas relacionados. ¿Necesitas datos deportivos? ESPN proporciona API para información de atletas, puntajes de juegos y más. Google tiene docenas de API en su sección de Desarrolladores para traducciones de idiomas, análisis, geolocalización y más.

## Pasos para la recopilación de información (flujo de trabajo)

Siguiendo a [@Iacus2015] que establece cinco pasos al momento de decidir recopilar información mediante el scraping web, estos pasos son:

1.  *Asegúrese de saber exactamente qué tipo de información necesita*. Esto puede ser específico (*el producto interno bruto de todos los países de la OCDE durante los últimos 10 años*) o vago (opinión de la gente sobre el teléfono de la empresa X, colaboración entre miembros del Senado de los Estados Unidos).
2. Averigüe si hay *fuentes de datos en la Web* que puedan proporcionar información *directa o indirecta* sobre su problema. Si está buscando hechos concretos, esto probablemente sea fácil. Si está interesado en conceptos bastante vagos, esto es más difícil.
3. Desarrolle una teoría del proceso de *generación de datos* cuando busque fuentes potenciales. ¿Cuándo se generaron los datos, cuándo se cargaron en la Web y quién lo hizo? ¿Existen áreas potenciales que no están cubiertas, son consistentes o precisas, y puede identificarlas y corregirlas? (*calidad*)
4. Equilibrar las *ventajas* y *desventajas* de las *posibles fuentes de datos*. Los aspectos relevantes pueden ser la *disponibilidad* (¡y la *legalidad*!), los *costos* de recolección, la *compatibilidad* de nuevas fuentes con la investigación existente, pero también factores *muy subjetivos* como la *aceptación* de la fuente de datos por parte de otros. También piense en posibles formas de *validar* la *calidad de sus datos.* ¿Existen otras fuentes independientes que brinden información similar para que sean posibles verificaciones cruzadas aleatorias? En caso de datos secundarios, ¿puede identificar la fuente original y verificar los errores de transferencia?
5. ¡Toma una decisión! Elija la fuente de datos que le parezca más adecuada, *documente* los motivos de la decisión y comience con los preparativos para la *recopilación*. Si es factible, recopile datos de *varias fuentes* para validar las fuentes de datos. Muchos problemas y beneficios de varias *estrategias de recopilación* de datos salen a la luz solo después de la recopilación real.

> Actividad (10 min): Buscar artículos de investigación (paper) que usen  el web scraping. Fuente de datos, objetivo, método.

  - Scielo: Activismo feminista en instagram. Argentina
  - Berkeley: Mercado inmobiliario en EEUU 
  - Revista Mexicana: Análisis de sentimiento sobre el Covid en twitter, México
  - U. de país Vasco: Rol de desinformación de influencer españoles en instagram

## Tecnologías de diseminación, extracción y almacenamiento Web

Una vez elegida la *fuente de datos* (página web) y al menos con la intuición de lo que se quiere obtener, el siguiente paso es decidir el mecanismos para el scraping (raspado), esto esta relacionado al *tipo de pagina web*, ver si esta ofrece una entrada *API* y conocer sus limitaciones puede ser un punto de partida. Esto se denomina *Technologies for disseminating, extracting, and storing web data Collecting*, en el marco del uso de R la figura \ref{scrap1} muestra las interacciones entre ellas.

![](_fig/scrap1.png)

*Tecnologías para difundir, extraer y almacenar datos web (considerando el entorno de R)*

## Librerías en R

Existen dos librerías muy completas para hacer el raspado:

  + **rvest:** Es bastante sencilla de utilizar, aprovecha el código html (código fuente) de la página web y genera dataframes de tablas web. (páginas estáticas)
  + **selenium:** Es útil cuando las páginas son dinámicas

El siguiente cuadro presenta las librerías en R relacionadas al web Scraping, incluyendo los servicios API, que a la fecha del proyecto alcanza a 761 librerías, esto representa el $5.02$% de las librerías en en R.

```{r,eval=F,message=FALSE}
library(knitr)
library(packagefinder)
library(xtable)
findPackage(c("API","scrape"),limit.results=-1)
```

> Actividad 2

Siguiendo los 5 pasos mencionados anteriormente, defina un tema de interés.

  1. Definir el tema:
  2. Identificar fuentes (web):
  3. Calidad de las fuentes:
  4. Ventajas y desventajas:
  5. Elegir la fuente:
  6. Realizar el raspado web de la información de interés

Para la clase del martes

## HTML

  + ¿Qué es?: *Lenguaje* de texto que se utiliza para *estructurar* y *desplegar* una página *web*
  + ¿Qué significa?: HyperText Markup Language
  + Regla/atributos: Funciona en términos de elementos, etiquetas y atributos. Tiene una estructura jerárquica.
    - Existe una jerarquía 
    - Funciona en base a *entornos* (encapsulamiento)
    - Dentro de los entornos existen atributos

## Librería rvest

Rvest es parte del universo **tidyverse** y esta orientada al scrape de páginas web. La instalación es usual:

```{r,eval=F}
#desde CRAN
install.packages("rvest")
install.packages("dplyr")
#la versión en desarrollo desde github
devtools::install_github("tidyverse/rvest")
```

Existe la herramienta selectorgadget disponible en http://selectorgadget.com/, esta permite interactuar con las páginas web para seleccionar partes del documento usando un CSS selector. Las funciones mas importantes dentro de rvest son:

  * **read_html**: para cargar la estructura de la página
  * **html_nodes**: para extraer información de la página según el CSS selector
  * **html_text**: para extraer texto de un html_nodes
  * **html_table**: para extraer tablas y ponerlas en data frame

## Ejemplo: Worldometers COVID

```{r}
rm(list = ls())
library(rvest)
library(dplyr)
library(xml2)
# importación
www<-"https://www.worldometers.info/coronavirus/"
bd<-read_html(www)
class(bd)
# raspado 
bd %>% html_nodes(".maincounter-number") %>% html_text()

bd %>% html_nodes(".maincounter-number") %>% html_text2()

bd %>% html_nodes(".maincounter-number") %>% html_nodes("span") %>% html_text2()

bdt<-bd %>% html_table()
t1<-bdt[[1]]
t2<-bdt[[2]]
t3<-bdt[[3]]
```

## Práctica por 2 puntos extra (MARTES):

- Ejercicio 1:

  1. Definir el tema:
  2. Identificar fuentes (web):
  3. Calidad de las fuentes:
  4. Ventajas y desventajas:
  5. Elegir la fuente:
  6. Realizar el raspado web de la información de interés

- Ejercicio 2: Usando la tabla más reciente del Worldometers COVID, mostrar los 5 países con más casos de muerte por millón por continente.

## Recomendaciones raspado web

## Ejemplo: Ketal

```{r}
rm(list = ls())
library(dplyr)
library(rvest)
library(xml2)
#################
www<-"https://www.ketal.com.bo/abarrotes"
ka<-read_html(www)
.product-layout
.price-normal
#ID PRODUCTO
aux<-ka %>% html_nodes(".product-layout") 
aux %>% html_nodes(".stats") %>% html_text2()
#marca
marca<-aux %>% html_nodes(".stats") %>% html_text2()

#id
id_prod<-aux %>% html_nodes(".stat-2") %>% html_text2()
#precio
aux %>% html_nodes(".price-normal") %>% html_text2()
aux %>% html_nodes(".price-new") %>% html_text2()
aux %>% html_nodes(".price") %>% html_children()
descuento<-aux %>% html_nodes(".price") %>% html_text2()
precio<-aux %>% html_nodes(".price-tax") %>% html_text2()
#descripcion
descripcion<-aux %>% html_nodes(".description") %>% html_text2()
aux %>% html_children() %>% html_children()
#base de datos
bd0<-data.frame(id_prod,descripcion,marca,descuento,precio)
```

Página completa de abarrotes

```{r}
www<-"https://www.ketal.com.bo/abarrotes?sort=p.sort_order&order=ASC&limit=100&page="

bda<-NULL
for(i in 1:8){
  print(i)
  ka<-read_html(paste0(www,i))
  aux<-ka %>% html_nodes(".product-layout")
  bd0<-data.frame(
    id = aux %>% html_nodes(".stat-2") %>% html_text2(),
    prod = aux %>% html_nodes(".description") %>% html_text2(),
    marca = aux %>% html_nodes(".stats") %>% html_text2(),
    precio = aux %>% html_nodes(".price-tax") %>% html_text2(),
    desc = aux %>% html_nodes(".price") %>% html_text2()
  )
  bda<-bda %>% bind_rows(bd0)
}

```

## Ejercicios Propuestos

1. Extraer la fecha, y el precio de compra y venta del dolar de la página https://www.bcb.gob.bo
2. Usando la página https://www.trabajopolis.bo/ seleccionar un departamento y armar una base de dato de ofertas laborales
3. Armar una base de datos en base a la página https://www.infocasas.com.bo
4. Explorar librarias API con acceso a Youtube y encontrar los 10 videos con más visualizaciones que incluyan a Bolivia en su titulo.
5. Explorar la libraria gtrendsR y explorar en que meses en Bolivia es mas frecuente la búsqueda de 
6. Usar la informacion de worldometers y generar un gráfico de contagios por millon de los distintos paises.